{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configurations available:\n",
      "\n",
      "    This is the configuration class to store the configuration of a [`LoraModel`].\n",
      "\n",
      "    Args:\n",
      "        r (`int`): Lora attention dimension.\n",
      "        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.\n",
      "        lora_alpha (`int`): The alpha parameter for Lora scaling.\n",
      "        lora_dropout (`float`): The dropout probability for Lora layers.\n",
      "        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out).\n",
      "            For example, gpt-2 uses `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set\n",
      "            to `True`.\n",
      "        bias (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the\n",
      "            corresponding biases will be updated during training. Be aware that this means that, even when disabling\n",
      "            the adapters, the model will not produce the same output as the base model would have without adaptation.\n",
      "        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable\n",
      "            and saved in the final checkpoint.\n",
      "        layers_to_transform (`Union[List[int],int]`):\n",
      "            The layer indexes to transform, if this argument is specified, it will apply the LoRA transformations on\n",
      "            the layer indexes that are specified in this list. If a single integer is passed, it will apply the LoRA\n",
      "            transformations on the layer at this index.\n",
      "        layers_pattern (`str`):\n",
      "            The layer pattern name, used only if `layers_to_transform` is different from `None` and if the layer\n",
      "            pattern is not in the common layers pattern.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "embeddings.word_embeddings: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.position_embeddings: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "embeddings.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.0.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.0.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.0.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.0.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.0.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.0.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.1.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.1.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.1.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.1.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.1.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.1.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.2.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.2.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.2.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.2.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.2.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.2.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.3.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.3.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.3.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.3.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.3.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.3.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.4.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.4.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.4.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.4.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.4.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.4.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.5.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.5.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "transformer.layer.5.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "transformer.layer.5.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.layer.5.ffn.activation: <class 'transformers.activations.GELUActivation'>\n",
      "transformer.layer.5.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "# Explorar capacidades de PEFT\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import transformers\n",
    "\n",
    "# Documentar configuraciones disponibles\n",
    "print(\"LoRA configurations available:\")\n",
    "print(LoraConfig.__doc__)\n",
    "\n",
    "# Explorar target_modules para diferentes architectures\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Model architecture:\")\n",
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) == 0:  # leaf modules\n",
    "        print(f\"{name}: {type(module)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
