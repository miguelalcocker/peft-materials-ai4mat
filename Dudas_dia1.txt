Tengo que preguntarte algunas dudas. :

Me has dicho que hiciera git checkout con la versión estable v0.5.0, pero no me has explicado ni justificado por qué. Lo digo por lo siguiente:

(ai4mat) malco@gpuserver2:~/peft-materials-ai4mat/external/peft$ git tag

v0.0.1

v0.0.2

v0.1.0

v0.10.0

v0.11.0

v0.11.1

v0.12.0

v0.13.0

v0.13.1

v0.13.2

v0.14.0

v0.15.0

v0.15.1

v0.15.2

v0.16.0

v0.2.0

v0.3.0

v0.4.0

v0.5.0

v0.6.0

v0.6.1

v0.6.2

v0.7.0

v0.7.1

v0.8.0

v0.8.1

v0.8.2

v0.9.0

Por último comentarte que estoy usando miniforge. Preguntar si es mejor que use anaconda, miniconda o alguna otra, o  si por el contrario así está bien:

(ai4mat) malco@gpuserver2:~/peft-materials-ai4mat/external/peft$ which conda

/home/malco/miniforge3/condabin/conda

(ai4mat) malco@gpuserver2:~/peft-materials-ai4mat/external/peft$ conda info

     active environment : ai4mat

    active env location : /home/malco/miniforge3/envs/ai4mat

            shell level : 1

       user config file : /home/malco/.condarc

 populated config files : /home/malco/miniforge3/.condarc

                          /home/malco/.condarc

          conda version : 25.5.1

    conda-build version : not installed

         python version : 3.12.10.final.0

                 solver : libmamba (default)

       virtual packages : __archspec=1=zen3

                          __conda=25.5.1=0

                          __cuda=12.8=0

                          __glibc=2.39=0

                          __linux=6.8.0=0

                          __unix=0=0

       base environment : /home/malco/miniforge3  (writable)

      conda av data dir : /home/malco/miniforge3/etc/conda

  conda av metadata url : None

           channel URLs : https://conda.anaconda.org/conda-forge/linux-64

                          https://conda.anaconda.org/conda-forge/noarch

          package cache : /home/malco/miniforge3/pkgs

                          /home/malco/.conda/pkgs

       envs directories : /home/malco/miniforge3/envs

                          /home/malco/.conda/envs

               platform : linux-64

             user-agent : conda/25.5.1 requests/2.32.1 CPython/3.12.10 Linux/6.8.0-63-generic ubuntu/24.04.2 glibc/2.39 solver/libmamba conda-libmamba-solver/25.3.0 libmambapy/2.1.1

                UID:GID : 1001:1001

             netrc file : /home/malco/.netrc

           offline mode : False

Por último mostrarte las salidas del gpu_capacity_test:

(ai4mat) malco@gpuserver2:~/peft-materiales-ai4mat$ python scripts/gpu_capacity_test.py > results/gpu_capacity_analysis.txt

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: resume_download is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use force_download=True.

  warnings.warn(

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.

  torch.utils._pytree._register_pytree_node(

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.

  torch.utils._pytree._register_pytree_node(

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: resume_download is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use force_download=True.

  warnings.warn(

config.json: 100%|███████████████████████████████████████████| 620/620 [00:00<00:00, 2.43MB/s]

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.

  torch.utils._pytree._register_pytree_node(

model.safetensors: 100%|███████████████████████████████████| 440M/440M [00:04<00:00, 96.4MB/s]

Some weights of BertModel were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']

You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

config.json: 100%|███████████████████████████████████████████| 631/631 [00:00<00:00, 2.48MB/s]

pytorch_model.bin: 100%|█████████████████████████████████| 13.7M/13.7M [00:00<00:00, 71.3MB/s]

Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']

You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

(ai4mat) malco@gpuserver2:~/peft-materiales-ai4mat$ cat results/gpu_capacity_analysis.txt 

=== Testing DistilBERT (baseline) ===

Model: distilbert-base-uncased

Model memory: 0.27 GB

Error: Target modules ['query', 'value'] not found in the base model. Please check the target modules and try again.

=== Testing MatSciBERT (materials science) ===

Model: m3rg-iitd/matscibert

Model memory: 0.44 GB

PEFT memory: 0.44 GB

Memory overhead: 0.00 GB

Trainable params: 294,912 (0.27%)

=== Testing ChemBERTa-77M (chemistry) ===

Model: DeepChem/ChemBERTa-77M-MLM

Model memory: 0.01 GB

PEFT memory: 0.01 GB

Memory overhead: 0.00 GB

Trainable params: 36,864 (1.06%)

=== SUMMARY ===

m3rg-iitd/matscibert: 0.44 GB (PEFT)

DeepChem/ChemBERTa-77M-MLM: 0.01 GB (PEFT)

Y del test_setup.py

(ai4mat) malco@gpuserver2:~/peft-materiales-ai4mat$ python scripts/test_setup.py 

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.

  torch.utils._pytree._register_pytree_node(

=== Testing GPU Setup ===

CUDA available: True

GPU: NVIDIA A40

GPU Memory: 47.7 GB

GPU tensor operations: OK

=== Testing Wandb ===

wandb: Currently logged in as: miguelalcocerperez (miguelalcocerperez-universidad-rey-juan-carlos) to https://api.wandb.ai. Use wandb login --relogin to force relogin

wandb: Tracking run with wandb version 0.21.0

wandb: Run data is saved locally in /home/malco/peft-materiales-ai4mat/wandb/run-20250728_162342-49gtvbyz

wandb: Run wandb offline to turn off syncing.

wandb: Syncing run setup-test-20250728-1623

wandb: ⭐️ View project at https://wandb.ai/miguelalcocerperez-universidad-rey-juan-carlos/AI4Mat-PEFT-Materials-2025

wandb: 🚀 View run at https://wandb.ai/miguelalcocerperez-universidad-rey-juan-carlos/AI4Mat-PEFT-Materials-2025/runs/49gtvbyz

wandb:                                                                                

wandb: 

wandb: Run history:

wandb: test_metric ▁

wandb: 

wandb: Run summary:

wandb: test_metric 42

wandb: 

wandb: 🚀 View run setup-test-20250728-1623 at: https://wandb.ai/miguelalcocerperez-universidad-rey-juan-carlos/AI4Mat-PEFT-Materials-2025/runs/49gtvbyz

wandb: ⭐️ View project at: https://wandb.ai/miguelalcocerperez-universidad-rey-juan-carlos/AI4Mat-PEFT-Materials-2025

wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)

wandb: Find logs at: ./wandb/run-20250728_162342-49gtvbyz/logs

Wandb logging: OK

=== Testing PEFT ===

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: resume_download is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use force_download=True.

  warnings.warn(

config.json: 100%|███████████████████████████████████████████| 483/483 [00:00<00:00, 2.11MB/s]

model.safetensors: 100%|███████████████████████████████████| 268M/268M [00:03<00:00, 72.1MB/s]

PEFT model created. Trainable parameters: 147456

PEFT basic functionality: OK

✅ All systems functional!

Y del notebook 01_peft_exploration:

/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(

LoRA configurations available:

    This is the configuration class to store the configuration of a [`LoraModel`].

    Args:
        r (`int`): Lora attention dimension.
        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.
        lora_alpha (`int`): The alpha parameter for Lora scaling.
        lora_dropout (`float`): The dropout probability for Lora layers.
        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out).
            For example, gpt-2 uses `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set
            to `True`.
        bias (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the
            corresponding biases will be updated during training. Be aware that this means that, even when disabling
            the adapters, the model will not produce the same output as the base model would have without adaptation.
        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable
            and saved in the final checkpoint.
        layers_to_transform (`Union[List[int],int]`):
            The layer indexes to transform, if this argument is specified, it will apply the LoRA transformations on
            the layer indexes that are specified in this list. If a single integer is passed, it will apply the LoRA
            transformations on the layer at this index.
        layers_pattern (`str`):
            The layer pattern name, used only if `layers_to_transform` is different from `None` and if the layer
            pattern is not in the common layers pattern.
    
/home/malco/miniforge3/envs/ai4mat/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Model architecture:
embeddings.word_embeddings: <class 'torch.nn.modules.sparse.Embedding'>
embeddings.position_embeddings: <class 'torch.nn.modules.sparse.Embedding'>
embeddings.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>
embeddings.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.0.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.0.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.0.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.0.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.0.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.0.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.1.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.1.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.1.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.1.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.1.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.1.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.2.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.2.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.2.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.2.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.2.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.2.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.3.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.3.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.3.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.3.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.3.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.3.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.4.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.4.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.4.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.4.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.4.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.4.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.5.attention.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.5.attention.q_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.attention.k_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.attention.v_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.attention.out_lin: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.sa_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>
transformer.layer.5.ffn.dropout: <class 'torch.nn.modules.dropout.Dropout'>
transformer.layer.5.ffn.lin1: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.ffn.lin2: <class 'torch.nn.modules.linear.Linear'>
transformer.layer.5.ffn.activation: <class 'transformers.activations.GELUActivation'>
transformer.layer.5.output_layer_norm: <class 'torch.nn.modules.normalization.LayerNorm'>

Por último decirte que he empezado a hacer la dockerización en local, porque como te comentaba antes en el servidor A40 no tengo sudo, y tampoco tengo podman ni docker, entonces he hecho la dockerización con ubuntu 22.04 y cuda 12.1, porque no he encontrado una versión 24.04 como en el servidor con cuda 12.1. No sé si es que no he buscado bien, o qué es lo que debería hacer.

